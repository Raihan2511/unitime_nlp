{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47f0cf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading quantized model (4-bit)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model & Tokenizer loaded.\n",
      "Trainable params: 2,654,208 / 154,757,376 (1.72%)\n",
      "üìÇ Loading and flattening dataset...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'output_text'],\n",
      "        num_rows: 1400\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'output_text'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'output_text'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "üß† Tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bc13c5b4a84ce682d15131373fa5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f063c26e92d14537a2f01a81776963bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279be946c3bd4b06b37327c2fae42602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "# fix_codet5p_q4_lora_final_fixed.py\n",
    "# Fine-tune CodeT5p-220M to generate XML outputs using 4-bit QLoRA with LoRA\n",
    "# Fixed version to handle 'context' field + flattening + tensor creation issues\n",
    "\n",
    "import os, gc, json, torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import DatasetDict, Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training,TaskType\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0Ô∏è‚É£  CUDA + Memory Configuration\n",
    "# ------------------------------------------------------------------\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1Ô∏è‚É£  Model + Tokenizer Setup\n",
    "# ------------------------------------------------------------------\n",
    "model_name = \"Salesforce/codet5p-220m\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Loading quantized model (4-bit)...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    # padding_side=\"left\",\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=True,\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"‚úÖ Model & Tokenizer loaded.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2Ô∏è‚É£  Prepare for LoRA (k-bit training)\n",
    "# ------------------------------------------------------------------\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"k\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params, all_param = 0, 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params:,} / {all_param:,} \"\n",
    "          f\"({100 * trainable_params / all_param:.2f}%)\")\n",
    "\n",
    "print_trainable_parameters(peft_model)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3Ô∏è‚É£  Dataset Loading (with flatten fix)\n",
    "# ------------------------------------------------------------------\n",
    "def flatten_jsonl(path):\n",
    "    fixed = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Combine fields cleanly\n",
    "            context = obj.get(\"context\", \"\")\n",
    "            prompt = obj.get(\"prompt\", \"\")\n",
    "            output = obj.get(\"output\", \"\")\n",
    "\n",
    "            # Flatten lists into strings\n",
    "            if isinstance(context, list): context = \" \".join(map(str, context))\n",
    "            if isinstance(prompt, list): prompt = \" \".join(map(str, prompt))\n",
    "            if isinstance(output, list): output = \" \".join(map(str, output))\n",
    "\n",
    "            # Create combined input text\n",
    "            input_text = f\"{context.strip()} {prompt.strip()}\".strip()\n",
    "\n",
    "            fixed.append({\n",
    "                \"input_text\": input_text,\n",
    "                \"output_text\": str(output),\n",
    "            })\n",
    "    return fixed\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"/home/sysadm/Music/unitime_nlp/data_generator/data/Courseofferings_dataset/train.jsonl\",\n",
    "    \"validation\": \"/home/sysadm/Music/unitime_nlp/data_generator/data/Courseofferings_dataset/validation.jsonl\",\n",
    "    \"test\": \"/home/sysadm/Music/unitime_nlp/data_generator/data/Courseofferings_dataset/test.jsonl\",\n",
    "}\n",
    "\n",
    "print(\"üìÇ Loading and flattening dataset...\")\n",
    "splits = {k: flatten_jsonl(v) for k, v in data_files.items()}\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_list(splits[\"train\"]),\n",
    "    \"validation\": Dataset.from_list(splits[\"validation\"]),\n",
    "    \"test\": Dataset.from_list(splits[\"test\"]),\n",
    "})\n",
    "print(dataset_dict)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4Ô∏è‚É£  Tokenization\n",
    "# ------------------------------------------------------------------\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 512\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        batch[\"output_text\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"üß† Tokenizing...\")\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input_text\", \"output_text\"],\n",
    ")\n",
    "print(\"‚úÖ Tokenization complete.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5Ô∏è‚É£  Evaluation Metrics\n",
    "# ------------------------------------------------------------------\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple) or (hasattr(preds, \"ndim\") and preds.ndim == 3):\n",
    "        pred_ids = np.argmax(preds, axis=-1)\n",
    "    else:\n",
    "        pred_ids = preds\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    exact_match = np.mean([p.strip() == l.strip() for p, l in zip(decoded_preds, decoded_labels)])\n",
    "    cer = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    decoded_labels_for_bleu = [[label] for label in decoded_labels]\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels_for_bleu)\n",
    "    return {\n",
    "        \"exact_match\": round(float(exact_match), 4),\n",
    "        \"cer\": round(float(cer), 4),\n",
    "        \"bleu\": round(float(bleu[\"score\"]), 4),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06bd635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     warmup_steps=1,\n",
    "#     weight_decay=0.01,\n",
    "#     learning_rate=2e-4,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=100,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=30,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=30,\n",
    "#     do_eval=True,\n",
    "#     gradient_checkpointing=True,\n",
    "#     load_best_model_at_end=False,\n",
    "#     metric_for_best_model=\"exact_match\",\n",
    "#     greater_is_better=True,\n",
    "#     fp16=True,\n",
    "#     report_to=\"none\",\n",
    "#     remove_unused_columns=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41488d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_326695/2310449507.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TrainingArguments configured.\n",
      "\n",
      "üî• Starting fine-tuning (memory-safe QLoRA)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='308' max='308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [308/308 17:29, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.665300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.045200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 6Ô∏è‚É£  Training Arguments (memory-safe)\n",
    "# ------------------------------------------------------------------\n",
    "output_dir = \"./Offereing-nlp-to-xml\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=4,\n",
    "    # per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=30,\n",
    "\n",
    "    # ‚úÖ Disable evaluation during training\n",
    "    eval_strategy=\"no\",\n",
    "    do_eval=False,  # ‚úÖ no evaluation\n",
    "\n",
    "    # ‚úÖ Remove or keep save only if you still want checkpoints\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=False,  # no eval, so no \"best model\"\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TrainingArguments configured.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7Ô∏è‚É£  Trainer\n",
    "# ------------------------------------------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model, padding=\"longest\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8Ô∏è‚É£  Training (with cleanup)\n",
    "# ------------------------------------------------------------------\n",
    "def pre_train_cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "pre_train_cleanup()\n",
    "\n",
    "print(\"\\nüî• Starting fine-tuning (memory-safe QLoRA)...\")\n",
    "trainer.train()\n",
    "print(\"üéâ Fine-tuning complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9ff5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (Your trainer.train() line)\n",
    "# print(\"üéâ Fine-tuning complete!\")\n",
    "\n",
    "# print(\"\\nüßπ Freeing VRAM... (deleting trainer and train data)\")\n",
    "# # --- THIS IS THE MEMORY FIX ---\n",
    "# del trainer\n",
    "# del tokenized_datasets[\"train\"] # We don't need the training set anymore\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# # --- END OF MEMORY FIX ---\n",
    "\n",
    "# print(\"‚úÖ VRAM cleared. Starting memory-safe manual evaluation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98ecca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This should now work without an OOM error\n",
    "# print(\"\\nüìä Running evaluation...\")\n",
    "# metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "# print(\"üìä Evaluation metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720000ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Running manual evaluation (memory-safe)...\n",
      "...Evaluation complete. Computing metrics.\n",
      "üìä Evaluation metrics: {'exact_match': 0.0, 'cer': 0.0291, 'bleu': 97.4163}\n"
     ]
    }
   ],
   "source": [
    "# (After trainer.train() and VRAM cleanup)\n",
    "\n",
    "print(\"\\nüìä Running manual evaluation (memory-safe)...\")\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "peft_model.eval()\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size=1, \n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "MAX_TARGET_LENGTH = 256 \n",
    "\n",
    "for batch in eval_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "    \n",
    "    labels = batch[\"labels\"].numpy() \n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = peft_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=MAX_TARGET_LENGTH,\n",
    "            num_beams=1,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Move generated IDs to CPU\n",
    "    preds = generated_ids.cpu().numpy() \n",
    "\n",
    "    \n",
    "    label_length = labels.shape[1]\n",
    "    \n",
    "    padded_preds = np.full((preds.shape[0], label_length), tokenizer.pad_token_id)\n",
    "    \n",
    "    current_pred_length = preds.shape[1]\n",
    "    \n",
    "    # Make sure we don't try to copy more than fits\n",
    "    copy_length = min(current_pred_length, label_length)\n",
    "    \n",
    "    # Copy the generated tokens into the padded array\n",
    "    padded_preds[:, :copy_length] = preds[:, :copy_length]\n",
    "    \n",
    "    # --- Padding Fix Ends Here Padding Fix ---\n",
    "    \n",
    "    all_preds.extend(padded_preds) \n",
    "    all_labels.extend(labels)       \n",
    "\n",
    "    # Aggressively clean up memory\n",
    "    del input_ids, attention_mask, labels, batch, generated_ids, preds, padded_preds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"...Evaluation complete. Computing metrics.\")\n",
    "metrics = compute_metrics((np.array(all_preds), np.array(all_labels)))\n",
    "print(\"üìä Evaluation metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680fbc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Salesforce/codet5p-220m\n",
      "Loading adapter from: /home/sysadm/Music/unitime_nlp/test/codet5p-finetuned-nlp-to-xml/checkpoint-308\n",
      "‚úÖ Model is ready for inference!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# --- 1. Define Model Names ---\n",
    "base_model_name = \"Salesforce/codet5p-220m\"\n",
    "adapter_path = \"/home/sysadm/Music/unitime_nlp/test/codet5p-finetuned-nlp-to-xml/checkpoint-308\"\n",
    "# --- 2. Load 4-bit Config ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# --- 3. Load Base Model (Quantized) ---\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically uses your GPU\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# --- 4. Load Tokenizer ---\n",
    "# (Must match the settings you trained with)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True,\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=True,\n",
    "    use_fast=False,\n",
    ")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- 5. Load the PEFT Adapter ---\n",
    "print(f\"Loading adapter from: {adapter_path}\")\n",
    "# This line merges your saved adapter onto the base model\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"‚úÖ Model is ready for inference!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5783112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Generating XML...\n",
      "\n",
      "--- Generated XML ---\n",
      "<offerings campus=\"woebegon\" year=\"2010\" term=\"Fal\" dateFormat=\"yyyy/M/d\" timeFormat=\"HHmm\" created=\"Sat Oct 18 19:33:17 CEST 2025\" includeExams=\"none\">\n",
      "  <offering id=\"132371\" offered=\"true\" action=\"insert\">\n",
      "    <course id=\"737984\" subject=\"CUBE\" courseNbr=\"106\" controlling=\"true\" title=\"CUBE_106\"/>\n",
      "    <config name=\"1\" limit=\"25\">\n",
      "      <subpart type=\"Lab\" suffix=\"\" minPerWeek=\"150\"/>\n",
      "      <class id=\"CUBE 10 Lab L1\" type=\"Lab\" suffix=\"L1\" limit=\"25\" studentScheduling=\"true\" displayInScheduleBook=\"true\" cancelled=\"false\" managingDept=\"0100\">\n",
      "        <time days=\"MWF\" startTime=\"0830\" endTime=\"0920\" timePattern=\"3 x 50\"/>\n",
      "        <room building=\"EDUC\" roomNbr=\"0830\"/>\n",
      "      </class>\n",
      "    </config>\n",
      "  </offering>\n",
      "</offerings>\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Define Input ---\n",
    "context = \"COURSE OFFERING REQUEST\"\n",
    "prompt_text = \"Add a new class: DLCS 10 (Deep Learning). Place it in EDUC on MWF between 0830 and 0920. It's a Lab with a limit of 25.\"\n",
    "\n",
    "# Format input just like training\n",
    "input_text = f\"{context.strip()} {prompt_text.strip()}\".strip()\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# --- 7. Generate ---\n",
    "print(\"...Generating XML...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,   # Your MAX_TARGET_LENGTH\n",
    "        num_beams=4,          # Use beam search for better results\n",
    "        early_stopping=False,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "xml_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n--- Generated XML ---\")\n",
    "print(xml_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff6589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203b43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
