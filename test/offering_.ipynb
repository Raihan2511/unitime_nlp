{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0cf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Loading quantized model (4-bit)...\n",
      "âœ… Model & Tokenizer loaded.\n",
      "Trainable params: 2,654,208 / 154,757,376 (1.72%)\n",
      "ðŸ“‚ Loading and flattening dataset...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'output_text'],\n",
      "        num_rows: 700\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'output_text'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'output_text'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n",
      "ðŸ§  Tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d95b9fdcb9b461384d646aed4d31997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7060128a434530a2fbacfd0350aa48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8226efbf794b4f768009e7bbd632bbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "# fix_codet5p_q4_lora_final_fixed.py\n",
    "# Fine-tune CodeT5p-220M to generate XML outputs using 4-bit QLoRA with LoRA\n",
    "# Fixed version to handle 'context' field + flattening + tensor creation issues\n",
    "\n",
    "import os, gc, json, torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import DatasetDict, Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training,TaskType\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0ï¸âƒ£  CUDA + Memory Configuration\n",
    "# ------------------------------------------------------------------\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1ï¸âƒ£  Model + Tokenizer Setup\n",
    "# ------------------------------------------------------------------\n",
    "model_name = \"Salesforce/codet5p-220m\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Loading quantized model (4-bit)...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    # padding_side=\"left\",\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=True,\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"âœ… Model & Tokenizer loaded.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2ï¸âƒ£  Prepare for LoRA (k-bit training)\n",
    "# ------------------------------------------------------------------\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"k\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params, all_param = 0, 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params:,} / {all_param:,} \"\n",
    "          f\"({100 * trainable_params / all_param:.2f}%)\")\n",
    "\n",
    "print_trainable_parameters(peft_model)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3ï¸âƒ£  Dataset Loading (with flatten fix)\n",
    "# ------------------------------------------------------------------\n",
    "def flatten_jsonl(path):\n",
    "    fixed = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Combine fields cleanly\n",
    "            context = obj.get(\"context\", \"\")\n",
    "            prompt = obj.get(\"prompt\", \"\")\n",
    "            output = obj.get(\"output\", \"\")\n",
    "\n",
    "            # Flatten lists into strings\n",
    "            if isinstance(context, list): context = \" \".join(map(str, context))\n",
    "            if isinstance(prompt, list): prompt = \" \".join(map(str, prompt))\n",
    "            if isinstance(output, list): output = \" \".join(map(str, output))\n",
    "\n",
    "            # Create combined input text\n",
    "            input_text = f\"{context.strip()} {prompt.strip()}\".strip()\n",
    "\n",
    "            fixed.append({\n",
    "                \"input_text\": input_text,\n",
    "                \"output_text\": str(output),\n",
    "            })\n",
    "    return fixed\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"/home/sysadm/Music/unitime_nlp/data/Courseofferings_dataset/train.jsonl\",\n",
    "    \"validation\": \"/home/sysadm/Music/unitime_nlp/data/Courseofferings_dataset/validation.jsonl\",\n",
    "    \"test\": \"/home/sysadm/Music/unitime_nlp/data/Courseofferings_dataset/test.jsonl\",\n",
    "}\n",
    "\n",
    "print(\"ðŸ“‚ Loading and flattening dataset...\")\n",
    "splits = {k: flatten_jsonl(v) for k, v in data_files.items()}\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_list(splits[\"train\"]),\n",
    "    \"validation\": Dataset.from_list(splits[\"validation\"]),\n",
    "    \"test\": Dataset.from_list(splits[\"test\"]),\n",
    "})\n",
    "print(dataset_dict)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4ï¸âƒ£  Tokenization\n",
    "# ------------------------------------------------------------------\n",
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 256\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        batch[\"output_text\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"ðŸ§  Tokenizing...\")\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input_text\", \"output_text\"],\n",
    ")\n",
    "print(\"âœ… Tokenization complete.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5ï¸âƒ£  Evaluation Metrics\n",
    "# ------------------------------------------------------------------\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple) or (hasattr(preds, \"ndim\") and preds.ndim == 3):\n",
    "        pred_ids = np.argmax(preds, axis=-1)\n",
    "    else:\n",
    "        pred_ids = preds\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    exact_match = np.mean([p.strip() == l.strip() for p, l in zip(decoded_preds, decoded_labels)])\n",
    "    cer = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    decoded_labels_for_bleu = [[label] for label in decoded_labels]\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels_for_bleu)\n",
    "    return {\n",
    "        \"exact_match\": round(float(exact_match), 4),\n",
    "        \"cer\": round(float(cer), 4),\n",
    "        \"bleu\": round(float(bleu[\"score\"]), 4),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41488d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4049717/15872788.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TrainingArguments configured.\n",
      "\n",
      "ðŸ”¥ Starting fine-tuning (memory-safe QLoRA)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 02:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.558600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 6ï¸âƒ£  Training Arguments (memory-safe)\n",
    "# ------------------------------------------------------------------\n",
    "output_dir = \"./codet5p-finetuned-nlp-to-xml\"\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     warmup_steps=1,\n",
    "#     weight_decay=0.01,\n",
    "#     learning_rate=2e-4,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=100,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=30,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=30,\n",
    "#     do_eval=True,\n",
    "#     gradient_checkpointing=True,\n",
    "#     load_best_model_at_end=False,\n",
    "#     metric_for_best_model=\"exact_match\",\n",
    "#     greater_is_better=True,\n",
    "#     fp16=True,\n",
    "#     report_to=\"none\",\n",
    "#     remove_unused_columns=False,\n",
    "# )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=30,\n",
    "\n",
    "    # âœ… Disable evaluation during training\n",
    "    eval_strategy=\"no\",\n",
    "    do_eval=False,  # âœ… no evaluation\n",
    "\n",
    "    # âœ… Remove or keep save only if you still want checkpoints\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=30,\n",
    "\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=False,  # no eval, so no \"best model\"\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"âœ… TrainingArguments configured.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7ï¸âƒ£  Trainer\n",
    "# ------------------------------------------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model, padding=\"longest\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8ï¸âƒ£  Training (with cleanup)\n",
    "# ------------------------------------------------------------------\n",
    "def pre_train_cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "pre_train_cleanup()\n",
    "\n",
    "print(\"\\nðŸ”¥ Starting fine-tuning (memory-safe QLoRA)...\")\n",
    "trainer.train()\n",
    "print(\"ðŸŽ‰ Fine-tuning complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff5cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Fine-tuning complete!\n",
      "\n",
      "ðŸ§¹ Freeing VRAM... (deleting trainer and train data)\n",
      "âœ… VRAM cleared. Starting memory-safe manual evaluation...\n"
     ]
    }
   ],
   "source": [
    "# # (Your trainer.train() line)\n",
    "# print(\"ðŸŽ‰ Fine-tuning complete!\")\n",
    "\n",
    "# print(\"\\nðŸ§¹ Freeing VRAM... (deleting trainer and train data)\")\n",
    "# # --- THIS IS THE MEMORY FIX ---\n",
    "# del trainer\n",
    "# del tokenized_datasets[\"train\"] # We don't need the training set anymore\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# # --- END OF MEMORY FIX ---\n",
    "\n",
    "# print(\"âœ… VRAM cleared. Starting memory-safe manual evaluation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Running evaluation...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This should now work without an OOM error\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š Running evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Š Evaluation metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# # This should now work without an OOM error\n",
    "# print(\"\\nðŸ“Š Running evaluation...\")\n",
    "# metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "# print(\"ðŸ“Š Evaluation metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "720000ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Running manual evaluation (memory-safe)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Evaluation complete. Computing metrics.\n",
      "ðŸ“Š Evaluation metrics: {'exact_match': 0.0, 'cer': 0.9827, 'bleu': 0.1836}\n"
     ]
    }
   ],
   "source": [
    "# (After trainer.train() and VRAM cleanup)\n",
    "\n",
    "print(\"\\nðŸ“Š Running manual evaluation (memory-safe)...\")\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "# Set model to evaluation mode\n",
    "peft_model.eval()\n",
    "\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size=1, \n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "# Get the max length you defined during tokenization\n",
    "MAX_TARGET_LENGTH = 256 \n",
    "\n",
    "for batch in eval_loader:\n",
    "    # Move inputs to GPU\n",
    "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "    \n",
    "    # Labels stay on CPU as numpy\n",
    "    labels = batch[\"labels\"].numpy() # Shape is (1, 256)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate predictions\n",
    "        generated_ids = peft_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=MAX_TARGET_LENGTH,\n",
    "            num_beams=1,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Move generated IDs to CPU\n",
    "    preds = generated_ids.cpu().numpy() # Shape is (1, variable_length)\n",
    "\n",
    "    # --- Padding Fix Starts Here Padding Fix ---\n",
    "    \n",
    "    # Get the length of the padded labels (e.g., 256)\n",
    "    label_length = labels.shape[1]\n",
    "    \n",
    "    # Create an empty numpy array filled with the pad token, \n",
    "    # matching the shape of the labels.\n",
    "    padded_preds = np.full((preds.shape[0], label_length), tokenizer.pad_token_id)\n",
    "    \n",
    "    # Find how many tokens were actually generated\n",
    "    current_pred_length = preds.shape[1]\n",
    "    \n",
    "    # Make sure we don't try to copy more than fits\n",
    "    copy_length = min(current_pred_length, label_length)\n",
    "    \n",
    "    # Copy the generated tokens into the padded array\n",
    "    padded_preds[:, :copy_length] = preds[:, :copy_length]\n",
    "    \n",
    "    # --- Padding Fix Ends Here Padding Fix ---\n",
    "    \n",
    "    all_preds.extend(padded_preds)  # Add the PADDED predictions\n",
    "    all_labels.extend(labels)       # Add the PADDED labels\n",
    "\n",
    "    # Aggressively clean up memory\n",
    "    del input_ids, attention_mask, labels, batch, generated_ids, preds, padded_preds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Now, compute metrics\n",
    "print(\"...Evaluation complete. Computing metrics.\")\n",
    "# This will now work, as both arrays will have shape (num_examples, 256)\n",
    "metrics = compute_metrics((np.array(all_preds), np.array(all_labels)))\n",
    "print(\"ðŸ“Š Evaluation metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
